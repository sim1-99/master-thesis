%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Methods} \label{chap:Methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{1cm}

% Type here an introduction to the chapter

\section{nnU-Net}


\section{GIN-IPA} \label{sec:gin-ipa}
To improve robustness of segmentation models against acquisition-induced domain shifts, the causality-inspired augmentation pipeline GIN-IPA\,\cite{Ouyang2023} was adopted. The method, aimed at single-source domain generalization (see Section \ref{sec:DomainGeneralization}), treats the image formation process as generated by two independent factors, acquisition $A$ and content $C$, and seeks to enforce that the segmentation predictor be invariant to interventions on $A$. Concretely, following the causal formulation, the desired invariance can be expressed as
\begin{equation}\label{eq:domain-inv}
    p\bigl(Y\mid S,\,\mathrm{do}(A= a_i)\bigr) 
    = p\bigl(Y\mid S,\,\mathrm{do}(A= a_j)\bigr), \quad \forall a_i,a_j,
\end{equation}
where $S$ denotes an ideal, domain-invariant representation determined by content $C$---i.e., shape information---and $Y$ is the segmentation mask. $p\bigl(Y\mid S,\,\mathrm{do}(A= a_i)\bigr)$ denotes the distribution that comes from letting images to be generated from a specific acquisition process $A = a_i$---being the same for another acquisition process $A = a_j$.

Because performing real interventions on $A$ is infeasible, GIN-IPA approximates such interventions by sampling photometric transformations $T(\cdot)$ that emulate different acquisition processes, and by enforcing consistency of network predictions across these simulated interventions.

The global intensity non-linear augmentation (GIN) module synthesizes a family of non-linear intensity and texture transforms that preserve anatomical geometry while producing diverse appearances (Fig.\,\ref{fig:gin_schema}). Each transform is instantiated as a shallow fully-convolutional network whose weights are sampled from isotropic Gaussian priors and whose inter-layer nonlinearity is a Leaky ReLU. The overall operator is expressed as
\begin{equation}\label{eq:gin}
    g_{\theta}(x) = \frac{\alpha\,g^{\mathrm{Net}}_{\theta}(x) + (1-\alpha)\,x}{\lVert \alpha\,g^{\mathrm{Net}}_{\theta}(x) + (1-\alpha)\,x \rVert_\mathrm{F}}\;\lVert x \rVert_\mathrm{F},
\end{equation}
where $g^{\mathrm{Net}}_{\theta}(\cdot)$ denotes the pure network output for random weights $\theta$, $\alpha\sim\mathcal{U}(0,1)$ is an interpolation coefficient and $\lVert\cdot\rVert_\mathrm{F}$ is the Frobenius norm. The normalization constrains the global energy of the augmented image, preventing global brightness or contrast changes from dominating the image. In other words, it ensures that anatomical shapes remain discernible while still allowing local texture and intensity variations to be introduced.

Key design choices include small receptive fields in the random convolutions (to avoid oversmoothing), linear interpolation with the original image (to retain semantics), and shallow depth (for computational efficiency and to avoid irrealistic results).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/gin_schema.png}
    \caption{Architecture of the GIN module. © 2022 IEEE, from \cite{Ouyang2023}.}
    \label{fig:gin_schema}
\end{figure}

Then, the interventional pseudo-correlation augmentation (IPA) addresses the shifted-correlation effect, whereby background unlabeled structures $X_\mathrm{b}$ become spuriously correlated with objects of interest $X_\mathrm{f}$ due to the acquisition process. IPA approximates the causal intervention $\mathrm{do}(X_\mathrm{f}=x_\mathrm{f})$---i.e., it removes the effects of the acquisition factor $A$ on $X_\mathrm{f}$---by resampling background appearances independently of the foreground through spatially-varying assignments of appearance transforms.

Let $g_{\theta_1}$ and $g_{\theta_2}$ be two independent GIN transforms sampled for the same input image, with number of channels $C$, height $H$ and width $W$. IPA creates a low-frequency pseudo-correlation map $b\in[0,1]$ with shape $(C \times H \times W)$ and blends the two GIN outputs as
\begin{equation}\label{eq:ipa}
    T_1\bigl(x;\theta_1,\theta_2,b\bigr) = g_{\theta_1}(x) \odot b + g_{\theta_2}(x) \odot (1-b),
\end{equation}
where $\odot$ denotes the Hadamard product, that is, element-wise multiplication. A complementary view $T_2(\cdot)$ is obtained by swapping $b$ and $1-b$. Pseudo-correlation maps are generated by interpolating a lattice of randomly-valued control points with cubic B-splines; in practice the control-point spacing is set to a fraction of the image dimension $\bigl(\textrm{e.g., }\frac{1}{4}\bigr)$ to maintain low spatial frequency and avoid shape distortion (Fig.\,\ref{fig:ipa_schema}).

By applying IPA to the entire image---both background and foreground---the pipeline avoids label-induced shortcuts and approximates sampling from independent background appearance distributions. This operation can be interpreted as assigning different photometric transformations to different spatial regions so that background and foreground appearances are de-correlated across training iterations.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/ipa_schema.png}
    \caption{Architecture of the IPA augmentation scheme (A), and construction principles of pseudo-correlation maps (B). © 2022 IEEE, from \cite{Ouyang2023}.}
    \label{fig:ipa_schema}
\end{figure}

In \cite{Ouyang2023} the training loop samples two GIN transforms and one pseudo-correlation map per image per iteration, generates the two IPA-augmented views $T_1(x)$ and $T_2(x)$, and computes the training loss. Alternative designs to B-spline in pseudo-correlation maps---e.g., random superpixels---were explored but found to be less effective. Both GIN and IPA steps are ruled by hyperparameters that must be set empirically. Among them, the most relevant (followed by the value that was used in \cite{Ouyang2023}, as a reference) are:
\begin{itemize}
    \item GIN
    \begin{itemize}
        \item number of layers of the shallow network (4)
        \item number of intermediate channels of the shallow network (2)
        \item interpolation coefficient $\alpha$ (uniform distribution)
    \end{itemize}
    \item IPA
    \begin{itemize}
        \item control point spacing in kernel matrix: it must be small enough in order to construct low-frequency pseudo-correlation maps $\bigl(\frac{1}{4}\textrm{ of the image length}\bigr)$
        \item interpolation order in kernel matrix: it serves as smoothing factor between the control points in the pseudo-correlation maps (2)
        \item control point value sampling (Gaussian distribution)
        \item downscale of the image: to reduce computational load (2)
    \end{itemize}
\end{itemize}
Default values were set by the authors of the paper by the combination of hyperparameter tuning, heuristic choices and hardware limitations.

Actually, practical optimizations include controlling the depth and width of the GIN networks. In \cite{Ouyang2023}, Ouyang and colleagues conducted hyperparameter tuning on the number of layers and channels in the GIN architecture, and on the sampling distribution of the interpolation coefficient $\alpha$. An ablation study on the contribution of the IPA step to the global performance was also conducted, showing improvements in all tested scenarios with respect to GIN-only training.

Finally, Ouyang and colleagues also participated in the FeTA 2022 challenge \cite{FeTA2022_review}. Their approach was based on training different nnU-Net models with varying data augmentation methods. GIN-IPA was employed in the augmentation pipeline for one of these models \cite{FeTA2022_top}. The models were subsequently ensembled, and the resulting segmentation method achieved the highest performance in the challenge, ranking first.

\section{Performance Metrics}
