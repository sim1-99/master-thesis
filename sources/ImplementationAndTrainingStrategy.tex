%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Implementation and Training Strategy} \label{chap:ImplementationAndTrainingStrategy}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{1cm}

The present chapter addresses the practical aspects of implementing the methods introduced earlier, focusing on model design choices, training procedures, and hyperparameter selection.

It begins with technical details of the nnU-Net architecture. The discussion then moves to the data augmentation strategies, which are the main distinguishing factor among the three models compared. Next, the training and validation strategy is described, followed by the hyperparameter tuning process, which identifies the optimal configuration for the shallow network used in GIN-IPA.

\section{Model Architecture} \label{sec:ModelArchitecture}
The model architecture is based on \texttt{nnU-Net v2.4.1}, equipped with the residual encoder preset \texttt{ResEncM}\,\cite{Isensee2024}. The configuration chosen is \texttt{3d-fullres}. The architecture involves \num{6} resolution stages for the encoder and the decoder, with a number of computational blocks of [\numlist[list-final-separator = {, }]{1;3;4;6;6;6}] per stage, respectively. Three layers are present in each computational block:
\begin{itemize}
    \item One convolution layer (\texttt{torch.nn.modules.conv.Conv3d}) with a kernel size of [\numlist[list-final-separator = {, }]{3;3;3}] and a stride of [\numlist[list-final-separator = {, }]{1;1;1}].
    \item One normalization layer (\texttt{torch.nn.InstanceNorm3d}).
    \item One non-linear activation function (\texttt{torch.nn.LeakyReLU}).
\end{itemize}
Between each encoder stage there is a downsampling layer, and between each decoder stage there is an upsampling layer. One convolution is performed between the output of each decoder stage and the skip connection from the corresponding stage in the encoder.

Further implementation details are:
\begin{itemize}
    \item \textbf{Initialization:} Kaiming uniform distribution (\texttt{torch.nn.init.kaiming\_unifo\\
    rm}).
    \item \textbf{Optimizer:} Stochastic gradient descent (\texttt{torch.optim.SGD}).
    \item \textbf{Learning Rate:} Initial learning rate set to \num{0.01}, following a polynomial schedule (\texttt{torch.optim.lr\_scheduler.PolynomialLR}).
    \item \textbf{Epochs:} Total of \num{1000} epochs, with \num{250} training iterations per epoch.
    \item \textbf{Loss Function:} \texttt{nnunetv2.training.loss.compound\_losses.DC\_and\_CE\_\\
    loss}, a weighted sum of Dice loss and \texttt{torch.nn.CrossEntropyLoss}. Dice loss optimizes the evaluation metric directly, but due to the patch based training, in practice merely approximates it. Combining the Dice loss with a cross-entropy loss improves training stability and segmentation accuracy\,\cite{Isensee2021}.
\end{itemize}

Moreover, the following rule-based parameters were established by nnU-Net:
\begin{itemize}
    \item \textbf{Batch Size:} \num{2}
    \item \textbf{Patch Size:} [\numlist[list-final-separator = {, }]{128;128;128}]
    \item \textbf{Features per Stage:} [\numlist[list-final-separator = {, }]{32;64;128;256;320;320}]
\end{itemize}
The dimensionality of both the input and output volumes is \num{3}. The default nnU-Net preprocessing was applied (non-zero cropping, \textit{z}-normalization).

In addition to the default nnU-Net data augmentation (see Section \ref{sec:nnUNet}), GIN-IPA was integrated into the training pipeline, in order to assess its contribution to robustness against domain shifts. Although the experiment described in \cite{Ouyang2023} is carried out with a 3D implementation, the method available online only supports 2D images. Therefore, starting from the 2D implementation, the code was adapted to work with 3D images, and integrated into the nnU-Net training pipeline. The version is publicly available at the GitHub repository \texttt{sim1-99/nnUNet-ginipa}\,\cite{nnUNet-ginipa}. However, because of hardware limitations, it was not possible to apply the IPA step in its 3D version---the background generation of the cubic B-splines required too much RAM memory. As a compromise, GIN transformation was applied to the whole volumes, while IPA was implemented through the product between a 2D pseudo-correlation map and each of the slices of one volume, along a randomly chosen axis.

Since the purpose of this work is to validate the effectiveness of GIN-IPA as an augmentation tool to improve model robustness against domain shifts in fetal MRI segmentation, the following three models were trained, in order to test their performance. \textit{p} represents the probability of application of a transformation to a volume; all of them were left to the default values defined in the nnU-Net framework.
\begin{enumerate}

    \item \textbf{Default nnU-Net DA}
    \begin{itemize}
        \item \textbf{Flip:} independently for each axis, $p=0.5$.
        \item \textbf{Rotation:} from \qtyrange{-30}{30}{\degree} independently for each axis, $p=0.2$.
        \item \textbf{Scaling:} from \qtyrange{70}{140}{\percent}, isotropically in 3D, $p=0.2$ (zoom in/out).
        \item \textbf{Gaussian noise:} $\sigma^2 \sim \mathcal{U}(0, 0.1)$, $p=0.1$.
        \item \textbf{Gaussian blur:} kernel $\sigma \sim \mathcal{U}(0.5, 1)$, $p=0.2$.
        \item \textbf{Multiplicative brightness:} from \qtyrange{75}{125}{\percent}, $p=0.15$.
        \item \textbf{Contrast:} from \qtyrange{75}{125}{\percent} (preserving the global intensity range), $p=0.15$.
        \item \textbf{Lower resolution:} downsample to a fraction \textit{f} of the original image---being $f \sim \mathcal{U}(0.5, 1)$---then resample to the original size, $p=0.25$.
        \item \textbf{Gamma transform:} $\gamma \sim \mathcal{U}(0.7, 1.5)$, preserving global intensity mean and standard deviation, $p=0.3$.
    \end{itemize}

    \item \textbf{GIN-IPA}
    \begin{itemize}
        \item \textbf{Flip:} independently for each axis, $p=0.5$.
        \item \textbf{Rotation:} from \qtyrange{-30}{30}{\degree} independently for each axis, $p=0.2$.
        \item \textbf{Scaling:} from \qtyrange{70}{140}{\percent}, isotropically in 3D, $p=0.2$ (zoom in/out).
        \item \textbf{GIN-IPA} ($p=0.5$):
        \begin{itemize}
            \item \textbf{GIN:}
            \begin{itemize}[label=$\diamond$]
                \item \textbf{Number of layers:} 2 (from hyperparameter tuning, Sec.\,\ref{sec:HyperparameterTuning}).
                \item \textbf{Intermediate channels:} 4 (from hyperparameter tuning, Sec.\,\ref{sec:HyperparameterTuning}).
                \item \textbf{Interpolation coefficient:} $\alpha \sim \mathcal{U}(0, 1)$.
            \end{itemize}
            \item \textbf{IPA:}
            \begin{itemize}[resume*]
                \item \textbf{Control point spacing:} $(64, 64)$.
                \item \textbf{Interpolation order:} \num{3}.
                \item \textbf{Control point values:} sampled from a Gaussian distribution.
                \item \textbf{Downscale factor:} \num{1} (no downscale).
            \end{itemize}
        \end{itemize}
    \end{itemize}

    \item \textbf{Default nnU-Net DA + GIN-IPA}
    \begin{itemize}
        \item Both methods applied with the same probabilities.
        \item Flip, rotation, and scaling applied once as first steps.
    \end{itemize}

\end{enumerate}

\section{Training and Validation Strategy} \label{sec:TrainingAndValidationStrategy}
Each of the three datasets---Kispi-mial, Kispi-irtk and dHCP---was split into training and test sets, with a ratio of \num{80}:\num{20}. The split was stratified both by age and pathology, to ensure that all classes were represented in both sets. In-training validation was not performed for two reasons:
\begin{itemize}
    \item \textbf{Limited data availability:} Kispi datasets were already small, and further splitting them would have resulted in insufficient data for training.
    \item \textbf{Focus on generalization:} The primary goal is not to achieve the best performing model on unseen data---albeit it is desirable---but rather to evaluate the model's performance on unseen data, which is better achieved by using a dedicated test set.
\end{itemize}

For each of the three aforementioned models, three independent trainings were carried out for each of the datasets, for a total of nine models. Then, for each trained model, independent segmentation predictions were made on the the three datasets: on test split of the \textit{in-domain} set, and on the other two \textit{out-of-domain}, unseen datasets. This experimental configuration allows to \enquote{isolate} the contribution of each DA method to the global model performance, and to assess the robustness of the model against domain shifts.

\section{Hyperparameter Tuning} \label{sec:HyperparameterTuning}
In \cite{Ouyang2023}, Ouyang and colleagues performed a hyperparameter tuning to determine the optimal configuration of the shallow network used in the GIN step. They conducted several trainings, varying either the number of layers (\numlist[list-final-separator={ or }]{2;4;8;16}) or the number of intermediate channels (\numlist[list-final-separator={ or }]{2;4;8;16}). In this study, their segmentation model was trained on abdominal CT images and tested on abdominal MRI images. The evaluation metric was limited to the Dice score, and the IPA step was not included. Their results indicated that the best configuration was achieved with \num{4} layers and \num{2} intermediate channels. The difference in Dice score between the best and worst configurations amounted to \num{7} when varying the number of layers, and \num{4} when varying the number of channels.

In the present analysis, adopting the configuration proposed by Ouyang and colleagues would have been bold. Although GIN-IPA is designed to be independent of the acquisition conditions, our focus is on testing it within the MRI domain, while explicitly including the IPA step, since it is a fundamental part of the method. Moreover, our evaluation is not limited to the Dice score, but also considers the volume similarity and the Hausdorff distance.

Hence, a new hyperparameter tuning on the GIN shallow network was conducted, using a grid search approach. The investigated configurations were:
\begin{itemize}
    \item \textbf{Number of layers:} \numlist[list-final-separator = {, }]{2;4;8}.
    \item \textbf{Number of intermediate channels:} \numlist[list-final-separator = {, }]{2;4;8}.
\end{itemize}
Configurations with \num{16} layers or \num{16} channels were not explored, in order to limit training time and because both led to the worst performance in \cite{Ouyang2023}.

In summary, the following tables report the average values per metric for nnU-Net---with GIN-IPA as DA technique, see Model 2.\ in Section \ref{sec:ModelArchitecture}---trained and tested as described in Section\,\ref{sec:TrainingAndValidationStrategy}. The best performance is highlighted in green, while the second-best is shown in light blue. Overall, the combination of \num{2} layers and \num{4} intermediate channels yielded the best results. This configuration was selected for the final model training, whose performance are analyzed in Chapter \ref{chap:Results}.

\begin{table}[htbp]
    \centering
    \begin{tabular}{c|c|c|c}
        \toprule
        \textbf{Average DC} & \num{2} channels & \num{4} channels & \num{8} channels \\
        \midrule
        \num{2} layers & \num{0.752} & \cellcolor{green!20}\num{0.753} & \num{0.748} \\
        \num{4} layers & \cellcolor{cyan!20}\num{0.753} & \num{0.751} & \num{0.746} \\
        \num{8} layers & \num{0.746} & \num{0.746} & \num{0.738} \\
        \bottomrule
    \end{tabular}

    \vspace{10pt}
    \begin{tabular}{c|c|c|c}
        \toprule
        \textbf{Average VS} & \num{2} channels & \num{4} channels & \num{8} channels \\
        \midrule
        \num{2} layers & \num{-0.114} & \cellcolor{cyan!20}\num{-0.103} & \cellcolor{green!20}\num{-0.098} \\
        \num{4} layers & \num{-0.134} & \num{-0.125} & \num{-0.106} \\
        \num{8} layers & \num{-0.154} & \num{-0.128} & \num{-0.104} \\
        \bottomrule
    \end{tabular}

    \vspace{10pt}
    \begin{tabular}{c|c|c|c}
        \toprule
        \textbf{Average HD95} & \num{2} channels & \num{4} channels & \num{8} channels \\
        \midrule
        \num{2} layers & \num{8.34} & \cellcolor{green!20}\num{8.33} & \cellcolor{cyan!20}\num{8.56} \\
        \num{4} layers & \num{9.10} & \num{9.00} & \num{8.71} \\
        \num{8} layers & \num{10.08} & \num{9.15} & \num{8.87} \\
        \bottomrule
    \end{tabular}
    \caption{Average evaluation metrics across configurations. The best result is colored in green, the second-best in light blue.}
\end{table}
