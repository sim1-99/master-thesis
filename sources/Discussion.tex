%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Discussion} \label{chap:Discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{1cm}

This chapter discusses the empirical evidence reported in Chapter \ref{chap:Results}, drawing a unified interpretation across general performance trends across datasets, pairwise model comparisons, and pathology-stratified analyses. The emphasis is on the behavior of the augmentation strategies in realistic in-domain and out-of-domain settings, and on the conditions under which gains are observed.

\section{Primacy of Data over Augmentation}
The first, and most consistent, observation is the primacy of the training dataset over the augmentation recipe. Models trained on dHCP achieve the most stable performance both ID and OOD, and do so \emph{independently} of whether augmentation is the nnU-Net default, GIN-IPA, or their combination. In Section \ref{sec:GeneralPerformance}, this is visible in the DSC histograms (Figs.\,\ref{fig:default_DC}--\ref{fig:both_DC}): while Kispi-trained models experience a marked OOD drop---particularly towards dHCP, with ventricles, dGM and WM most affected---the dHCP-trained models exhibit limited degradation across inference domains. Moreover, no DSC gain emerges from switching augmentation when training on dHCP. Together, these findings imply that \emph{dataset quality and scale} dominate generalization in this setting.

A further confirmation of the importance of the data quality emerges from Kispi-mial: the change of domain does not impact the network trained on Kispi-mial in the same manner as the other two sources. Concretely, the performance turns out to be higher OOD than ID for two tissues, and only when inferring on Kispi-irtk. Even though the mechanism is not clear, this is probably attributable to two factors:
\begin{itemize}
    \item It may be supposed that the lower quality of Kispi-mial\,\cite{FeTA2021_review} prevents the network from learning robust features. This makes it perform better on a dataset---i.e., Kispi-irtk---that is similar to the source, but has a higher image quality, which inherently drives to better segmentations.
    \item On the other hand, the small size of the ID test set of Kispi-mial---only \num{8} samples---limits the reliability of the results, which may be strongly influenced by the specific cases included. In other words, the lower ID performance may be due to a few challenging cases in the test set, which are not representative of the whole dataset.
\end{itemize}
However, the global DSC values still reflect the trends observed in the other two training cases, with ID performance being higher than OOD.

\section{Efficacy of GIN-IPA}
\paragraph{Baseline vs.\ GIN-IPA.}
When training on Kispi-irtk and inferring on dHCP, GIN-IPA yields a \emph{substantial improvement} over the baseline across metrics and labels. At the global level, the average DSC increases $(\qty{+58}{\percent})$, with concordant improvements in VS $(\qty{-36}{\percent})$ and HD95 $(\qty{-22}{\percent})$. The corresponding paired analysis confirms the statistical significance and a large effect size for DSC in this cross-domain transfer (DSC: \numrange[range-open-phrase=from\ ]{0.34}{0.54}, $p \ll 0.01$, $|d|=1.1$; VS: \numrange[range-open-phrase=from\ ]{1.12}{0.71}, $p\ll 0.01$, $|d|=0.8$; HD95: \numrange[range-open-phrase=from\ ]{49}{38}, $p\ll 0.01$, $|d|=0.6$). The increase is particularly pronounced for dGM (DSC: \numrange[range-open-phrase=from\ ]{0.18}{0.59}), ventricles (DSC: \numrange[range-open-phrase=from\ ]{0.11}{0.42}), and BS (DSC: \numrange[range-open-phrase=from\ ]{0.43}{0.70}). The full set of paired comparisons for all metrics and labels (including VS and HD95) is reported in Appendix \hyperref[app:SupplementaryTables]{B}.

Conversely, training on dHCP shows no material benefit from GIN-IPA over the baseline across any inference domain or label. For Kispi-mial, effects are small and structure-dependent, with modest improvements predominantly in cases where the baseline struggles on poorly segmented volumes.

\paragraph{GIN-IPA vs.\ Combined Augmentation.}
Stacking the two augmentation methods does not systematically improve performance over GIN-IPA alone, but rather can make it worse. Actually, when trained on Kispi-irtk and inferred on dHCP, the combined strategy is consistently inferior to pure GIN-IPA for several structures (CSF, cerebellum, dGM) across all metrics. Training on dHCP again yields no differences between the two.

\section{Robustness by Pathology}
The stratified analysis (Section \ref{sec:PerformanceByPathology}) indicates that models trained on dHCP generalize equally well to healthy subjects in Kispi-mial and Kispi-irtk, despite the domain shift (different image quality and reconstruction techniques). Performance decreases for pathological cases, reflecting segmentation challenges in the presence of anatomical abnormalities. In this more difficult regime, GIN-IPA achieves very small improvements over the baseline, while remaining broadly equivalent to the combined strategy. Furthermore, these results are explained with the concentration of the lower-quality images in Kispi-mial among the pathological scans; this is the reason for the larger performance gap in that subgroup, once again confirming the relevance of data quality in this segmentation framework.

\section{General Outcomes}
The evidence above supports three general conclusions:
\begin{itemize}
    \item \textbf{Data quality and scale prevail:} When training data are abundant and homogeneous (like in dHCP), the choice of an augmentation pipeline among those tested barely alters performance, because the performance is already high for every model (thanks to the inherent robustness of nnU-Net). The converse is also true: with smaller, noisier sources (such as Kispi), OOD degradation is pronounced.
    \item \textbf{GIN-IPA is conditionally beneficial:} Its gains are largest in the domain generalization scenario from Kispi-irtk to dHCP, where it closes a considerable part of the OOD gap. However, the effects shrink or vanish when the source dataset is large enough and of high quality to cover the target variability.
    \item \textbf{Stacking augmentations is not additive:} The combined strategy often overlaps with, and can even dilute, the benefits of GIN-IPA; it never consistently outperforms GIN-IPA in the examined cross-domain settings.
\end{itemize}

These patterns are coherent with the intended role of GIN-IPA: by synthesizing intensity and spatial variations, it exposes the network to harder, more diverse views of a limited source, which is most valuable when the source lacks the target domain variability. Once data already cover the relevant distributional modes---as with dHCP---marginal augmentation gains become negligible.

\section{Limitations and Future Work}
Three principal aspects delimit the scope of the present findings. First, the small size of the Kispi cohorts constrained further stratification (e.g., finer gestational age, training by pathology) and limited the precision of subgroup estimates. Generally speaking, the scarcity of public datasets, together with their high intra-variability, jeopardizes the possibility to totally isolate single domains. Second, label-set harmonization between Kispi and dHCP required a mapping (Tab.\,\ref{tab:label_merge}), which, although carefully defined, introduces an additional layer of variability in label-wise comparisons. Third, because of hardware limitations, the IPA step was performed in its 2D variant, which may be less effective than the full 3D version.

Practically, the results argue for prioritizing \emph{data curation}---larger, multi-center, high-quality fetal MRI with standardized SRR pipelines---since source quality and scale are the dominant predictors of cross-domain success in this task. Within constrained-data regimes, GIN-IPA is a \emph{useful augmentation choice}, particularly for single-source DG from moderately sized, relatively clean sources (e.g., Kispi-irtk). Nonetheless, stacking it with standard nnU-Net augmentation is unnecessary and sometimes counterproductive. For challenging pathological cases, improved acquisition and reconstruction remain central to performance gains.

Besides data, it could be interesting to disentangle the two components of GIN-IPA---possibly employing the 3D IPA variant and broader datasets that allow a more rigorous cross-domain validation---to assess their individual contributions, and to explore other augmentation strategies (e.g., adversarial, style-transfer) in this setting.
