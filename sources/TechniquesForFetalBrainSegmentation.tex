%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Techniques for Fetal Brain Segmentation} \label{chap:TechniquesForFetalBrainSegmentation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{1cm}

Automated segmentation of the fetal brain is a central task in the analysis of prenatal MRI, furnishing an anatomical framework for studying brain development and enabling quantitative measurements. Manual annotation, while considered the reference standard, is time-consuming, requires expert knowledge, and suffers from inter-observer variability. Consequently, a wide range of computational approaches have been developed to achieve accurate, reproducible, and efficient segmentation of fetal brain structures.

Over the years, segmentation methods have evolved from traditional atlas-based strategies to modern deep learning approaches. Convolutional neural networks have emerged as the dominant paradigm, outperforming classical approaches while remaining sensitive to differences in acquisition protocols, scanners, and populations. Domain generalization addresses the challenge of adapting models to these variations, ensuring robust performance across diverse imaging conditions.

This chapter reviews the main deep learning methods for fetal brain segmentation, introducing the concept of domain generalization and presenting the state of the art. Ultimately, it outlines the most recent results of the FeTA Challenge, which provides a benchmark for evaluating segmentation methods in a standardized setting.

\vspace{-5pt}

\section{Domain Generalization} \label{sec:DomainGeneralization}
In medical image segmentation, a \emph{domain} encapsulates both the feature space and the underlying marginal distribution of imaging data. Domain shifts arise when models trained on one domain are tested on data from another dataset. In medical imaging, the most prevalent sources of domain shift arise from variations in image acquisition processes, encompassing differences in imaging modalities, scanning protocols, and device manufacturers---a more exhaustive description of the potential sources can be found in Section \ref{sec:Challenges}. This is frequently referred to as \enquote{acquisition shift}. Unlike data generalization, where test samples share the same distribution as training data, domain generalization addresses the scenario where test data arise from a distinct, unseen domain\,\cite{Ouyang2023}.

Domain generalization (DG) methods can be categorized into three main groups, which are often complementary and can be combined to achieve higher performance\,\cite{Wang2023}.

\paragraph{Data Manipulation.}
This group of techniques focuses on manipulating input data to increase the diversity and quantity of existing training data.
\begin{description}
    \item[Data Augmentation-Based DG] Involves applying various transformations to training data to simulate different domain characteristics and reduce overfitting. Domain randomization is a common technique where new data is generated by introducing random variations in parameters such as object location, texture, shape, number, illumination, camera view, and noise. An example in medical imaging is SynthSeg\,\cite{Billot2023}, which generates synthetic images with randomized contrasts based on Gaussian mixture models (GMMs), along with geometric deformations and resampling to simulate different image resolutions. FetalSynthSeg\,\cite{Billot2023} further extends this by incorporating intensity clustering and meta-classes for fetal brain MRI. Another approach is adversarial data augmentation, which generates image perturbations that are specifically designed to easily flip the predictions of classifiers, thereby making the model more robust to such changes.
    \item[Data Generation-Based DG] Consists in creating diverse and rich synthetic data to enhance the model generalization capabilities. Techniques often leverage generative models such as variational auto-encoders (VAEs) and generative adversarial networks (GANs), or strategies like Mixup\,\cite{Zhang2018}. These methods can be complex due to their computational demands and the careful design required for the generative models.
\end{description}

\paragraph{Representation Learning.}
This category aims to learn feature representations that are robust to domain shifts. The core idea is to decompose the prediction function into a feature extraction (representation learning) function and a classifier function, focusing on making the feature extraction robust.
\begin{description}
    \item[Domain-Invariant Representation Learning] Seeks to reduce the discrepancies between feature distributions across multiple source domains. The underlying principle is that if feature representations remain invariant to different domains, they are inherently more general and transferable to unseen domains. This involves methods like kernel-based methods\,\cite{Blanchard2021}, domain adversarial learning---e.g., domain-adversarial neural networks, DANN \cite{Ganin2015, Ganin2016}---and invariant risk minimization\,\cite{Krueger2021}.
    \item[Feature Disentanglement-Based DG] Aims to separate learned features into domain-shared features---which are common across domains and useful for the task---and domain-specific features---which capture domain-specific variations. In this category, causality-inspired methods aim to ensure that the learned representations capture the \enquote{true cause} of the labels (e.g., object shape) and are therefore unaffected by correlated but irrelevant features like background, color, or style. Causality-inspired methods have been used to achieve single-source domain generalization through data augmentation. The idea is to simulate interventions on irrelevant features, thereby exposing the network to synthetic acquisition-shifted examples.
\end{description}

\paragraph{Single-Source Domain Generalization (\textsc{ssdg})}
This is a specific and highly challenging setting within domain generalization where only a single source domain is available for training. This scenario is particularly common in medical imaging applications due to the high cost of data collection, privacy concerns, and scarcity of diverse datasets\,\cite{Ouyang2023}. The key to this problem is to generate novel domains using data generation techniques to increase the diversity and informativeness of training data\,\cite{Wang2023}. The performance degradation that deep learning models face due to domain shift can be attributed to\,\cite{Ouyang2023}:
\begin{description}
    \item[Shifted Domain-Dependent Features] Image appearance, such as intensity and texture, is inherently domain-dependent. Deep networks are susceptible to shifts in these features; in contrast, human annotators can readily identify anatomical structures across different domains by focusing on domain-invariant shape information, which is intuitively causal to segmentation masks, unlike intensity or texture.
    \item[Shifted-Correlation Effect] Due to confounding variables, objects in the background of an image may be spuriously correlated with the objects of interest, rather than causally related. For example, a network might learn that a certain background artifact (which, in the case of fetal MRI, would be the skull and the maternal tissues) is correlated with a specific anatomical structure within a source domain. If this artifact is absent or appears differently in an unseen target domain, the network's reliance on this spurious correlation can lead to failure.
\end{description}
The goal of \textsc{ssdg} is to mitigate these effects by steering the network towards learning domain-invariant features, such as shape information, and immunizing the segmentation model against the shifted-correlation effect by removing confounders during training. This is exactly the idea behind GIN-IPA\,\cite{Ouyang2023}, that is discussed in greater detail in Section \ref{sec:Challenges}.

\section{Convolutional Neural Networks}
Convolutional neural networks (CNNs) represent a cornerstone among deep learning models for medical image segmentation. Their design is inherently suited to the spatial structure of imaging data: instead of operating on an entire image at once, CNNs employ local filters that slide across the image volume, detecting meaningful spatial patterns such as edges, boundaries, textures, and intensity gradients.

A CNN is constructed as a hierarchy of processing stages. Early layers extract lower-level features, capturing fine structural elements such as tissue interfaces and local contrast changes. As depth increases, subsequent layers integrate these features into increasingly abstract representations that encode global spatial context.

Besides the global context, prediction tasks like tissue segmentation require methods that also preserve spatial detail. Encoder-decoder architectures---such as in U-Net---are designed to meet this requirement. In the encoder, feature maps are progressively downsampled through a sequence of convolutions that reduce spatial resolution while increasing the number of feature channels. However, this hierarchical compression inevitably comes at the cost of fine-grained spatial detail. Downsampling blurs or discards the subtle boundaries that separate closely adjacent tissues. To recovering this high-frequency information, the decoder mirrors the encoder with a sequence of upsampling stages that progressively restore the spatial resolution lost during encoding. At each resolution level, the upsampled feature maps are enriched with context learned at coarser scales, allowing the model to reconstruct tissue boundaries in a globally coherent manner. The U-Net architecture introduces skip connections that link each encoder stage to its corresponding decoder stage. Skip connections transmit the high-resolution spatial information extracted early in the encoder directly to the decoder. This operation allows the decoder to leverage both global context and local detail simultaneously, preserving sharp boundaries. In fetal MRI, skip connections are particularly beneficial for preserving delicate interfaces such as the outer contour of the cortical plate, the borders of the deep gray matter, and the thin structure of the brainstem. In fetal MRI, where tissue contrast may be low, motion artefacts frequent, and partial-volume effects prominent, U-Net's ability to integrate global and local information is essential.

Three-dimensional U-Nets (illustrated in Fig\,\ref{fig:unet}) extend this scheme to volumetric data by applying convolutions and feature extraction operations in three spatial dimensions. Instead of treating each slice independently, the network processes the entire volume (or patches of it), enabling it to capture inter-slice continuity and structural coherence.

The U-Net architecture\,\cite{Ronneberger2015} has thus become the reference model for biomedical image segmentation and forms the backbone of nnU-Net, investigated in this thesis. Although 3D models impose greater computational demands, frameworks such as nnU-Net automatically adapt patch size, depth, and preprocessing to the characteristics of each dataset, providing a strong and reproducible baseline across diverse clinical and research settings\,\cite{Isensee2021}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/unet.png}
    \caption{Schematic 3D U-Net architecture. Â© 2011 LMB, University of Freiburg, from \cite{Ronneberger2015}.}
    \label{fig:unet}
\end{figure}

Building on the generic description of U-Net architectures, recent work on fetal brain MRI segmentation has converged toward increasingly sophisticated convolutional architectures that explicitly target the specific challenges of in-utero imaging. These challenges include heterogeneous image quality and acquisition protocols, rapid gestational changes in anatomy and tissue contrast, and the presence of structural abnormalities and motion artefacts.

Early work by Khalili and colleagues\,\cite{Khalili2019} demonstrated the feasibility of automatic multi-tissue segmentation in reconstructed fetal brain MRI using convolutional neural networks, showing clear gains over traditional atlas-based and intensity-driven methods but still struggling with generalization across gestational ages and atypical anatomies. Subsequent contributions have focused on alleviating annotation bottlenecks and improving robustness. Fetit and colleagues\,\cite{Fetit2020} proposed a deep learning framework for cortical grey-matter segmentation that explicitly addresses the scarcity of high-quality manual labels\,\cite{Fetit2020}. Their system operates on volumetric T2-weighted reconstructions from the dHCP fetal cohort and adopts a 3D multi-pathway CNN with parallel convolutional streams operating at different resolutions. Instead of relying on fully manual 3D annotations, they exploit segmentations generated by the neonatal Draw-EM pipeline to train an initial network, and then refine the model by incorporating corrections on fewer than \num{300} carefully selected 2D slices. This human-in-the-loop strategy dramatically reduces expert annotation workload\,\cite{Fetit2020}.

Beyond single-tissue or cortical-only approaches, more recent architectures explicitly aim at comprehensive multi-tissue parcellation. CAS-Net (Conditional Atlas Segmentation Network) couples a 3D U-Net-style structure with a conditional atlas branch\,\cite{Li2022}. The network predicts tissue label maps and a subject-specific atlas. The atlas enables the model to learn anatomical priors without depending solely on the intensity values of the input image. This can improve the segmentation performance especially if there is no gold standard label for training due to the poor image quality. On a nine-label fetal brain parcellation, CAS-Net achieves an overall Dice similarity coefficient of approximately \qty{85}{\percent}, highlighting the benefits of embedding a learned anatomical prior directly into the CNN architecture\,\cite{Li2022}.

Another line of work tackles the intrinsic anisotropy and plane-dependent contrast of in-utero acquisitions by exploiting multi-view information. \textsc{irmmnet} (Inception Residual Multi-view Multi-tissue Network) processes axial, coronal, and sagittal slices through parallel 2D encoder-decoder streams, and fuses the resulting features for joint segmentation and gestational age prediction\,\cite{Mazher2022}. The segmentation branch targets multi-tissue fetal brain parcellation on the FeTA dataset\,\cite{Payette2021}, while a regression head estimates gestational age from the shared encoder representations. Multi-view fusion allows the network to combine complementary information across orthogonal planes, mitigating ambiguities that arise when segmenting individual 2D stacks independently. \textsc{irmmnet} demonstrates that multi-view 2D CNNs can compete with fully 3D architectures while being computationally lighter and more flexible with respect to slice thickness and coverage\,\cite{Mazher2022}.

While the architectures above focus primarily on improving average segmentation accuracy, Fidon and colleagues\,\cite{Fidon2021} explicitly address robustness to rare but clinically critical cases. Using 3D nnU-Net as a backbone, they investigate the problem of hidden stratification in multi-tissue fetal brain segmentation---i.e., the tendency of models trained to maximize average performance to fail on under-represented subpopulations, such as fetuses with open spina bifida or other severe malformations. Their dataset consists of reconstructed T2-weighted 3D fetal brain volumes from the FeTA dataset and a private dataset\,\cite{Fidon2021}. Standard nnU-Net training yields high mean Dice scores but exhibits catastrophic failures for some abnormal cases, particularly in structures like cerebellum and white matter. To mitigate this, they replace empirical risk minimization---the default in nnU-Net---with a distributionally robust optimization objective that emphasizes hard examples via hardness-weighted sampling during training. The resulting nnU-Net-DRO significantly improves the lower percentiles of the Dice score distribution---for instance, increasing the worst-case performance on cerebellar segmentation in spina bifida---without degrading performance\,\cite{Fidon2021}.

More recently, CasUNeXt introduced a cascaded 2D CNN tailored to the multi-view and multi-site nature of fetal MRI\,\cite{Zhigao2024}. The framework comprises a global localization network (Loc-Net) and a fine segmentation network (Seg-Net), both based on encoder-decoder structures but enhanced with separable convolutions and attention gates. The Loc-Net first identifies the brain region in full-field 2D slices across axial, coronal, and sagittal views; the input volume is then cropped around the localized brain and fed to Seg-Net for high-resolution tissue segmentation. Then, the separable convolutions reduce the computational cost, while attention mechanisms selectively emphasize informative features and suppress background or artefact-related responses. On multi-view datasets comprising normal and abnormal fetal brains, CasUNeXt consistently outperforms the U-Net baseline in both localization and segmentation\,\cite{Zhigao2024}. Notably, qualitative experiments show that CasUNeXt maintains accurate delineations in the presence of severe motion artefacts and maternal tissue interference where standard U-Net architectures generate substantial false negatives and false positives\,\cite{Zhigao2024}.

Taken together, these CNN-based approaches delineate the current state of the art in fetal brain MRI segmentation. Architecturally, most methods are rooted in U-Net-like encoder-decoder designs, either fully 3D or multi-view 2D, but they incorporate additional mechanisms: learned anatomical priors (CAS-Net), multi-task learning (\textsc{irmmnet}), cascaded localization-segmentation (CasUNeXt), and robustness-oriented training objectives (nnU-Net-DRO).

\section{The FeTA Challenge}
The Fetal Tissue Annotation Challenge (FeTA)\,\cite{FeTA2024} was born in 2020, and joined the International Conference on Medical Image Computing and Computer-Assisted Intervention (\textsc{miccai})\,\cite{MICCAI} in 2021. Up to now, four editions have been organized (in 2020, 2021, 2022, and 2024), with increasing participation and interest from the medical imaging community. The main contributions of the FeTA Challenge are the creation of a benchmark dataset for fetal brain MRI segmentation and biometry, and the promotion of the development of algorithms for the automatic segmentation of fetal brain tissues.

The main task in FeTA is the segmentation of brain tissues in fetal MRI, which is a challenging problem due to the low contrast between tissues, the presence of noise, and the variability in the shape and size of the fetal brain. The dataset used in the challenge is composed of 3D super-resolution (SR) reconstructions of 2D fetal brain MRI images. Participants are asked to segment the fetal brain into seven tissues: external cerebrospinal fluid (CSF), cortical gray matter (cGM), white matter (WM), ventricles (including cavum), cerebellum, deep gray matter (dGM), and brainstem (BS). The performance is evaluated using three metrics: the Dice similarity coefficient (DSC), the volume similarity (VS), and the Hausdorff 95 distance (HD95). The use of three metrics helps to reduce the reliance on any one metric, which may be misleading in the evaluation of the algorithms\,\cite{FeTA2024_paper}.

The first edition of the FeTA Challenge was organized in 2020, by Payette et al.\,\cite{Payette2021}. The challenge consisted in segmenting fetal brain MRI T2w images. The initial FeTA dataset comprised \num{40} super-resolution (SR) reconstructions with manual segmentations for training and \num{10} SR reconstructions without manual segmentation for validation, encompassing both pathological and non-pathological cases. The gestational age (GA) range spanned from \numrange{20}{33} weeks. This dataset established a standard in fetal brain tissue parcellation---according to the seven-tissues protocol previously introduced in\,\cite{Payette2020}---that would be used in all the following FeTA editions. Four research groups participated, submitting a total of ten algorithms. All the algorithms had more or less the same issues in segmenting the CSF---especially for the pathological cases, because of not clear tissue boundaries---and the GM, because of its rapidly changing structure.
The dataset used in the first FeTA edition had important limitations:
\begin{itemize}
    \item Manual segmentations were based on a single segmentation due to time and resource limitations, without consensus delineation.
    \item The data were from one single center, the University Children's Hospital Zurich (Kispi), thus limiting the generalizability of the results.
    \item The images had varying quality grades, with younger GAs and pathological cases often having lower quality.
\end{itemize}

The 2021 edition of the FeTA Challenge\,\cite{FeTA2021_review, FeTA2021} was the first to join the \textsc{miccai} conference. The dataset---hereinafter referred to as Kispi dataset---was expanded to \num{120} scans from the same institution, with GAs ranging from \numrange{20}{35} weeks. The acquisition was carried out at \qty{1.5}{\tesla} for a subset of cases, and at \qty{3}{\tesla} for another subset of cases. \num{60} scans were reconstructed with the \textsc{mialsrtk} method\,\cite{Tourbier2015, MIALSRTK}, while the other \num{60} cases with the \textsc{simple irtk} method\,\cite{Kuklisova2012, irtk-simple}. For each reconstruction method, \num{40} cases were included in the training dataset available to the challenge participants (for a total of \num{80} cases), and \num{20} cases were included in testing dataset not available to the participants (for a total of \num{40} cases). \num{21} algorithms were submitted, of which \num{19} were U-Nets, with no major differences in the architecture. Overall, the most challenging labels to segment were cortical and deep GM---due to limited image resolution and annotation uncertainty---and brainstem---especially in the pathological cases. The results of the image quality and SR reconstruction methods are related to each other, as the majority of the low quality images were done with the \textsc{mialsrtk} method, and the excellent quality brain volumes included were reconstructed with the \textsc{simple irtk} method.

FeTA 2022\,\cite{FeTA2022_review, FeTA2022} introduced a multi-center dataset to address the generalizability of algorithms, which was one of the main limitations of the previous editions. In addition to Kispi, data from Medical University of Vienna was incorporated into both the training and testing datasets. Data from two further centers were included in the testing dataset---University Hospital Lausanne (\textsc{chuv}), and Benioff Children's Hospital (UC San Francisco, \textsc{ucsf})---for a total of four centers.
\comment{
(Tab\,\ref{tab:FeTA2022_dataset}).

\begin{table}[htbp]
    \centering
    \begin{tabular}{c|c|c|c|c}
        \toprule
        \textbf{Inst.} & \begin{tabular}{@{}c@{}} \textbf{Scanner} \\ \textbf{(field strength in Tesla)} \end{tabular} & \textbf{SRR method} & \begin{tabular}{@{}c@{}} \textbf{TR/TE} \\ \textbf{(ms)} \end{tabular} & \begin{tabular}{@{}c@{}} \textbf{GA range} \\ \textbf{(weeks)} \end{tabular} \\ \midrule
        \multicolumn{5}{c}{\textbf{Training}} \\
        \midrule
        Kispi & \begin{tabular}{@{}c@{}} GE Signa Discovery \\ MR450/MR750 (1.5/3)* \end{tabular} & \begin{tabular}{@{}c@{}} \textsc{mialsrtk} (40) \\ \textsc{simple irtk} (40) \end{tabular} & \begin{tabular}{@{}c@{}} 2000-3500 \\ 120** \end{tabular} & 20.0-34.8 \\ \hline
        Vienna & \begin{tabular}{@{}c@{}} Philips Ingenia/Intera (1.5) \\ Philips Achieva (3) \end{tabular} & \textsc{NiftyMIC} (40) & \begin{tabular}{@{}c@{}} 6000-22000 \\ 80-140 \end{tabular} & 19.3-34.4 \\
        \midrule
        \multicolumn{5}{c}{\textbf{Testing}} \\
        \midrule
        Kispi & \begin{tabular}{@{}c@{}} GE Signa Discovery \\ MR450/MR750 (1.5/3)* \end{tabular} & \begin{tabular}{@{}c@{}} \textsc{mialsrtk} (20) \\ \textsc{simple irtk} (20) \end{tabular} & \begin{tabular}{@{}c@{}} 2000-3500 \\ 120** \end{tabular} & 21.3-34.6 \\ \hline 
        Vienna & \begin{tabular}{@{}c@{}} Philips Ingenia/Intera (1.5) \\ Philips Achieva (3) \end{tabular} & \textsc{NiftyMIC} (40) & \begin{tabular}{@{}c@{}} 6000-22000 \\ 80-140 \end{tabular} & 18.1-35.0 \\ \hline
        \textsc{chuv} & \begin{tabular}{@{}c@{}} Siemens \textsc{magnetom} \\ Aera (1.5) \end{tabular} & \textsc{mialsrtk} (40) & \begin{tabular}{@{}c@{}} 1200 \\ 90 \end{tabular} & 21.0-35.0 \\ \hline
        \textsc{ucsf} & \begin{tabular}{@{}c@{}} GE Discovery \\ MR750/MR750W (3) \end{tabular} & \textsc{NiftyMIC} (40) & \begin{tabular}{@{}c@{}} 2000-3500 \\ 100** \end{tabular} & 20.0-35.1 \\
        \bottomrule
    \end{tabular}
    \caption{Training and testing dataset properties in FeTA 2022. In parenthesis, next to each SRR method, is reported the correspondent number of images. *The field strengths respectively refers to the scanners. **TE values represent the minimum durations.}
    \label{tab:FeTA2022_dataset}
    \end{table}
}

\num{17} algorithms were submitted, among which nnU-Net was the most used and effective tool. Overall, the median performance metrics in the OOD setting remained equivalent to the in-domain, but for some labels---ventricles, GM and  WM---an important drop in performance was observed. The most challenging labels to segment remained cortical and deep GM, and BS. Notably, some algorithms performed better in the OOD setting than in the in-domain setting. This can be explained with the better quality of the images from the \textsc{chuv} and \textsc{ucsf} centers, which were included only in the test set. Style and photometric augmentations (contrast, blur, sharpness, etc.) turned out to be effective in improving the generalization of the models. However, \enquote{the optimum choice of augmentation techniques remains unclear}\,\cite{FeTA2022_review}, standing as a critical factor in achieving domain generalization. The paper traces a path for future research, highlighting that it should focus on enhancing the generalizability of the methods and that \enquote{conducting a more comprehensive evaluation of the impact of data augmentation and possible biases due to super-resolution reconstruction methods would be very valuable}\,\cite{FeTA2022_review}.

Finally, in FeTA 2024\,\cite{FeTA2024_review, FeTA2024} \num{20} new scans were added to the test set, in order to have more results on the OOD performance. These scans were acquired at St.\ Thomas Hospital (King's College London, KCL), with field strength of \qty{0.55}{\tesla} (Siemens \textsc{magnetom} Free.Max)\,\cite{FeTA2024_paper}. This decision follows the recent rise in popularity of low-cost low-field MRI systems\,\cite{Aviles2023}, which are particularly suitable for fetal imaging due to their lower SAR and acoustic noise\,\cite{Ponrartana2023}. \num{16} algorithms were submitted, of which nine were based on nnU-Net\,\cite{nnUNet, Isensee2021}. The top team used an nnU-Net with a denoising autoencoder, generating ensemble predictions from different models. The second top team used a custom U-Net variant, training it on real and synthetic data (SynthSeg\,\cite{Billot2023}), and applying post-processing to discard non-brain tissues from the predictions. All top-three teams applied extensive data augmentation, combinations of standard augmentations, and model ensembling.

Across the leading methods, average DSC plateaued between \numlist{0.80;0.82}, likely due to the quality of both SRR algorithms and manual segmentations\,\cite{FeTA2022_review}. No statistically significant improvement was observed in overall segmentation performance metrics over the last three editions, suggesting that a performance plateau has been reached despite the increasing sophistication of methodologies and dataset diversity. These results indicate that merely architectural modifications are unlikely to produce significant improvements, consistent with observations from other challenges in which U-Net-based methods frequently outperform more complex designs\,\cite{Eisenmann2023, FeTA2024_review}. Notably, the low-field MRI dataset from KCL achieved the highest segmentation performance among all sites. Conversely, the Kispi dataset, in spite of being an in-domain dataset, exhibited the lowest performance. Although domain shifts are widely recognized as a key challenge for deep learning methods in medical imaging\,\cite{Dockes2021}, the sources of these shifts are rarely disentangled. Analysis of domain shifts revealed that image quality was the most influential factor affecting model generalization, leading to Dice score differences of up to \num{0.10} between low- and high-quality scans. The choice of the SRR pipeline also exerted a substantial impact on segmentation performance, highlighting the need for better modeling of artifacts specific to fetal brain SR pipelines\,\cite{Sanchez2024a}. Other factors, such as gestational age, pathology, and acquisition site, contributed marginally to performance variability.

\comment{
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/feta24_in-domain.png}
    \includegraphics[width=0.8\textwidth]{figures/feta24_out-of-domain.png}
    \caption{Teams' Dice similarity scores in FeTA 2024. From\,\cite{FeTA2024}.}
    \label{fig:FeTA2024_results}
\end{figure}
}